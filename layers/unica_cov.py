# layers/unica_cov.py
import torch
import torch.nn as nn
import torch.nn.functional as F


class UniCAFiLM(nn.Module):
    """
    åå˜é‡åŒè´¨åŒ– + FiLM/æ®‹å·® èåˆ + ç¨³å®šé—¨æ§
    ä½œç”¨å¯¹è±¡ï¼šTimer-XL çš„ token åµŒå…¥ [B, C*N, D]
    åå˜é‡ï¼šåŸå§‹å¤šå˜é‡ x ä¸æ—¶é—´æ ‡è®° x_markï¼ˆå¯é€‰ï¼‰
    å…³é”®ï¼šæ’ç­‰èµ·æ­¥ã€å¼±é—¨æ§ã€å…¨å±€æ··åˆç³»æ•°ã€æ²¿ token çš„è½»é‡å¹³æ»‘
    å…¼å®¹ï¼šä¸é‡å ä¸é‡å  patchï¼ˆstride <= patch_lenï¼‰
    """
    def __init__(
            self,
            d_model: int,
            bottleneck: int = 128,
            input_token_len: int = 24,       # patch_len (P)
            exclude_target: bool = False,
            fusion: str = "film_gate",       # "film_gate" æˆ– "res_add"
            gamma_scale: float = 0.1,        # Î³ æ‰°åŠ¨å¹…åº¦
            beta_scale: float  = 0.05,       # Î² ä½ç§»å¹…åº¦ï¼ˆfilm_gateï¼‰
            res_scale: float   = 0.1,        # Î” æ®‹å·®å¹…åº¦ï¼ˆres_addï¼‰
            dropout: float     = 0.0,        # æ¡ä»¶åˆ†æ”¯æ­£åˆ™
            init_gate_bias: float  = -2.0,   # gate åˆå§‹å°
            init_alpha_bias: float = -2.0,   # Î± åˆå§‹å° (sigmoid å ~0.12)
            smooth_gate_ks: int = 3,         # gate çš„ token å¹³æ»‘æ ¸(å¥‡æ•°)
            smooth_beta_ks: int = 3          # Î² çš„ token å¹³æ»‘æ ¸(å¥‡æ•°)
    ):
        super().__init__()
        assert fusion in ["film_gate", "res_add"]
        self.d_model = d_model
        self.bottleneck = bottleneck
        self.input_token_len = input_token_len
        self.exclude_target = exclude_target
        self.fusion = fusion
        self.gamma_scale = float(gamma_scale)
        self.beta_scale  = float(beta_scale)
        self.res_scale   = float(res_scale)
        self.smooth_gate_ks = int(smooth_gate_ks)
        self.smooth_beta_ks = int(smooth_beta_ks)

        # æ¡ä»¶åˆ†æ”¯
        self.proj_in  = nn.LazyLinear(bottleneck)   # [*, F] -> [*, B]
        self.ln       = nn.LayerNorm(bottleneck)
        self.drop     = nn.Dropout(dropout)

        if fusion == "film_gate":
            self.proj_out = nn.Linear(bottleneck, 2 * d_model)  # -> [Î³_pred, Î²_pred]
            self.gate     = nn.Linear(bottleneck, 1)
        else:
            self.proj_out = nn.Linear(bottleneck, d_model)       # -> Î”

        # å…¨å±€æ··åˆç³»æ•°ï¼ˆé€šè¿‡ sigmoid åˆ° 0~1ï¼‰
        self.alpha = nn.Parameter(torch.tensor(init_alpha_bias))

        # === å…³é”®åˆå§‹åŒ–ï¼šæ’ç­‰èµ·æ­¥ & å¼±é—¨æ§ ===
        nn.init.zeros_(self.proj_out.weight)
        nn.init.zeros_(self.proj_out.bias)
        if fusion == "film_gate":
            nn.init.zeros_(self.gate.weight)
            nn.init.constant_(self.gate.bias, init_gate_bias)

    @torch.no_grad()
    def _window_pool(self, x: torch.Tensor, x_mark: torch.Tensor, target_channel: int,
                     n_tokens: int):
        """
        å°† x/x_mark å¯¹é½åˆ° token çº§ï¼šæ¯å˜é‡æ¯ token ç»™ä¸€ä¸ªæ‘˜è¦ï¼ˆå‡å€¼/æ–¹å·® + å¹³å‡æ—¶é—´ç‰¹å¾ï¼‰
        å…¼å®¹é‡å åˆ‡ç‰‡ï¼šæ ¹æ® n_tokens åæ¨ strideï¼Œå¹¶åœ¨å¿…è¦æ—¶åˆ‡/è¡¥åˆ° n_tokens
        x      : [B, L, C]
        x_mark : [B, L, M] æˆ– None
        return : [B, C*n_tokens, F_cov]
        """
        B, L, C = x.shape
        P = int(self.input_token_len)
        N = int(n_tokens)

        # åæ¨ strideï¼ˆN=1 æ—¶é€€åŒ–ä¸ºæ•´æ®µçª—å£ï¼‰
        if N <= 1:
            step = P
        else:
            # æœŸæœ›æ»¡è¶³ N = floor((L - P)/step) + 1  => step = (L - P)/(N - 1)
            # å–æ•´åå†åšä¸€æ¬¡ä¸€è‡´æ€§ä¿®æ­£
            num = max(0, L - P)
            step = max(1, num // (N - 1))
        # æŒ‰æ¨æ–­ step unfold
        x_var = x.permute(0, 2, 1).unfold(dimension=-1, size=P, step=step)   # [B,C,N',P]
        Np = x_var.shape[2]
        # è‹¥ N' ä¸æœŸæœ› N ä¸ä¸€è‡´ï¼Œåˆ™è£/è¡¥åˆ° N
        if Np < N:
            pad_cnt = N - Np
            last = x_var[:, :, -1:, :].expand(-1, -1, pad_cnt, -1)
            x_var = torch.cat([x_var, last], dim=2)
        elif Np > N:
            x_var = x_var[:, :, :N, :]

        mu  = x_var.mean(dim=-1, keepdim=False)                               # [B,C,N]
        std = x_var.var(dim=-1, keepdim=False, unbiased=False).add(1e-6).sqrt()
        stat = torch.stack([mu, std], dim=-1).view(B, C * N, 2)               # [B,C*N,2]

        # å¯é€‰ï¼šæ’é™¤ç›®æ ‡é€šé“
        if self.exclude_target and (0 <= target_channel < C):
            stat_ = stat.view(B, C, N, 2)
            if C > 1:
                others = torch.cat([stat_[:, :target_channel], stat_[:, target_channel+1:]], dim=1)  # [B,C-1,N,2]
                repl = others.mean(dim=1, keepdim=True).expand(-1, 1, -1, -1)                        # [B,1,N,2]
                stat_[:, target_channel:target_channel+1] = repl
            stat = stat_.view(B, C * N, 2)

        # æ—¶é—´æ ‡è®°æ± åŒ–å¹¶æ‹¼æ¥ï¼ˆç”¨åŒæ ·çš„ P/stepï¼Œå¹¶åš N å¯¹é½ï¼‰
        if x_mark is not None:
            xm = x_mark.unfold(dimension=1, size=P, step=step).mean(dim=2)    # [B,N',M]
            Npm = xm.shape[1]
            if Npm < N:
                pad_cnt = N - Npm
                last = xm[:, -1:, :].expand(-1, pad_cnt, -1)
                xm = torch.cat([xm, last], dim=1)
            elif Npm > N:
                xm = xm[:, :N, :]
            xm = xm.unsqueeze(1).expand(B, C, N, -1).contiguous().view(B, C * N, -1)
            cov_feat = torch.cat([stat, xm], dim=-1)                          # [B,C*N,2+M]
        else:
            cov_feat = stat                                                   # [B,C*N,2]
        return cov_feat

    def _smooth_1d(self, t: torch.Tensor, k: int) -> torch.Tensor:
        """å¯¹ [B*C, D, N] æˆ– [B*C, 1, N] åš 1D å‡å€¼å¹³æ»‘ï¼ˆk ä¸ºå¥‡æ•°ï¼‰"""
        if k <= 1:
            return t
        pad = k // 2
        return F.avg_pool1d(t, kernel_size=k, stride=1, padding=pad)

    def forward(self, embed_tokens: torch.Tensor, x: torch.Tensor, x_mark: torch.Tensor,
                target_channel: int):
        """
        embed_tokens : [B, C*N, D]  â€”â€” ä¸»åˆ†æ”¯ token åµŒå…¥ï¼ˆTimer-XL çš„ Linear/Encoder åï¼‰
        x            : [B, L, C]    â€”â€” å»ºè®®ä¼ â€œæœªå½’ä¸€åŒ–â€çš„åŸå§‹åºåˆ—ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
        x_mark       : [B, L, M] æˆ– None
        return       : [B, C*N, D]  â€”â€” èåˆåçš„ token åµŒå…¥
        """
        B, CN, D = embed_tokens.shape
        assert x.dim() == 3, f"x shape={x.shape}"
        Bx, L, C = x.shape
        assert Bx == B, f"batch mismatch: {Bx}!={B}"

        # ğŸ”§ æ ¸å¿ƒå…¼å®¹ï¼šä»å¼ é‡æ¨æ–­ Nï¼ˆæ”¯æŒé‡å  patchï¼‰
        assert CN % C == 0, f"shape mismatch: CN({CN}) not divisible by C({C})"
        N = CN // C

        # æ„é€ ä¸ tokens å¯¹é½çš„åå˜é‡ç‰¹å¾
        cov_feat = self._window_pool(x, x_mark, target_channel, n_tokens=N)   # [B, C*N, F]

        # æ¡ä»¶åˆ†æ”¯
        h = self.proj_in(cov_feat)                                            # [B, C*N, Bk]
        h = self.ln(F.gelu(h))
        h = self.drop(h)

        mix = torch.sigmoid(self.alpha)                                       # å…¨å±€æ··åˆç³»æ•° âˆˆ (0,1)

        if self.fusion == "film_gate":
            film = self.proj_out(h)                                           # [B, C*N, 2D]
            gamma_pred, beta = film.chunk(2, dim=-1)                          # [B,C*N,D], [B,C*N,D]
            gamma = 1.0 + self.gamma_scale * torch.tanh(gamma_pred)
            beta  = self.beta_scale * torch.tanh(beta)                        # é™åˆ¶ä½ç§»å¹…åº¦

            # --- æ²¿ token ç»´å¹³æ»‘ beta ---
            if self.smooth_beta_ks > 1:
                beta_bcnd = beta.view(B * C, N, D).transpose(1, 2)            # [B*C, D, N]
                beta_bcnd = self._smooth_1d(beta_bcnd, self.smooth_beta_ks)   # [B*C, D, N]
                beta = beta_bcnd.transpose(1, 2).contiguous().view(B, C * N, D)

            y = embed_tokens * gamma + beta

            gate = torch.sigmoid(self.gate(h))                                # [B, C*N, 1]
            # --- æ²¿ token ç»´å¹³æ»‘ gate ---
            if self.smooth_gate_ks > 1:
                g = gate.view(B * C, N, 1).transpose(1, 2)                    # [B*C,1,N]
                g = self._smooth_1d(g, self.smooth_gate_ks)                   # [B*C,1,N]
                gate = g.transpose(1, 2).contiguous().view(B, C * N, 1)

            out = embed_tokens + (mix * gate) * (y - embed_tokens)            # æ®‹å·®é—¨æ§ + å…¨å±€æ··åˆ
            return out
        else:  # "res_add"
            delta = self.proj_out(h)                                          # [B, C*N, D]
            out = embed_tokens + mix * (self.res_scale * delta)
            return out
